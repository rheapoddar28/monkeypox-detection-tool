{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e3ef2d",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d3be3",
   "metadata": {},
   "source": [
    "# Preparing the training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec64707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 770 files [00:00, 2043.48 files/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import splitfolders\n",
    "\n",
    "splitfolders.ratio(\"../Datasets/Original/\", # The location of dataset\n",
    "                   output=\"../Datasets/Datasets_Divided/\", # The output location\n",
    "                   seed=22, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of splited dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8cf34",
   "metadata": {},
   "source": [
    "###  Real-time Data Augmentation through ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc614a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_generator = ImageDataGenerator(rotation_range=90, \n",
    "                                     brightness_range=[0.1, 0.7],\n",
    "                                     width_shift_range=0.5, \n",
    "                                     height_shift_range=0.5,\n",
    "                                     horizontal_flip=True, \n",
    "                                     vertical_flip=True,\n",
    "                                     preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa161d1",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66370c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "datagen = ImageDataGenerator(        \n",
    "            rotation_range=45,\n",
    "            width_shift_range=0.2,  \n",
    "            height_shift_range=0.2,\n",
    "            brightness_range=[0.1, 0.7],\n",
    "            shear_range=0.2,        \n",
    "            zoom_range=0.3,        \n",
    "            horizontal_flip=True, \n",
    "            vertical_flip=True,\n",
    "            fill_mode='nearest', cval=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091e50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_directory='../Datasets/Datasets_Divided/train/'\n",
    "base_directory_target='../Datasets/Augmented/train/'\n",
    "disease=os.listdir(base_directory)\n",
    "\n",
    "for y in range(4):\n",
    "    images=os.listdir(base_directory+str(disease[y]))[1:5]\n",
    "    dataset=[]\n",
    "    for i ,image_name in enumerate(images):\n",
    "        if(image_name.split('.')[1]=='png'):\n",
    "            image=io.imread(base_directory+str(disease[y])+'/'+image_name)\n",
    "            image=Image.fromarray(image,'RGB')\n",
    "            dataset.append(np.array(image))\n",
    "\n",
    "    x=np.array(dataset)\n",
    "    i = 1\n",
    "    for batch in datagen.flow(x, batch_size=1,save_to_dir=base_directory_target+str(disease[y]),save_prefix='aug',save_format='png'):        \n",
    "        i += 1\n",
    "        if i > (4*(x.shape[0])):  \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6a6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_directory='../Datasets/Datasets_Divided/val/'\n",
    "base_directory_target='../Datasets/Augmented/val/'\n",
    "disease=os.listdir(base_directory)\n",
    "\n",
    "for y in range(4):\n",
    "    images= os.listdir(base_directory+str(disease[y]))\n",
    "    dataset=[]\n",
    "    for i ,image_name in enumerate(images):\n",
    "        if(image_name.split('.')[1]=='png'):\n",
    "            image=io.imread(base_directory+str(disease[y])+'/'+image_name)\n",
    "            image=Image.fromarray(image,'RGB')\n",
    "            dataset.append(np.array(image))\n",
    "\n",
    "    x=np.array(dataset)\n",
    "    i = 1\n",
    "    for batch in datagen.flow(x, batch_size=1,save_to_dir=base_directory_target+str(disease[y]),save_prefix='aug',save_format='png'):        \n",
    "        i += 1\n",
    "        if i > (4*(x.shape[0])):  \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db65a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Normal', 'Monkeypox', 'Measles', 'Chickenpox']\n",
      "['Normal', 'Monkeypox', 'Measles', 'Chickenpox']\n",
      "['Normal', 'Monkeypox', 'Measles', 'Chickenpox']\n"
     ]
    }
   ],
   "source": [
    "train_data_dir=\"../Datasets/Augmented/train/\"\n",
    "test_data_dir=\"../Datasets/Datasets_Divided/test/\"\n",
    "val_data_dir=\"../Datasets/Augmented/val/\"\n",
    "\n",
    "class_names=sorted(os.listdir(\"../Datasets/Augmented/train/\"), reverse=True)[:4]\n",
    "class_names_test=sorted(os.listdir(\"../Datasets/Datasets_Divided/test/\"), reverse=True)[:4]\n",
    "class_names_val = sorted(os.listdir(\"../Datasets/Augmented/val/\"), reverse=True)[:4]\n",
    "print(class_names)\n",
    "print(class_names_test)\n",
    "print(class_names_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7435e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_subset = sorted(os.listdir(\"../Datasets/Augmented/train/\"), reverse=True)[:4] # Using only the first 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0494a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5179 images belonging to 4 classes.\n",
      "Found 900 images belonging to 4 classes.\n",
      "Found 81 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "traingen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               subset='training',\n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n",
    "\n",
    "validgen = train_generator.flow_from_directory(val_data_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n",
    "\n",
    "testgen = test_generator.flow_from_directory(test_data_dir,\n",
    "                                             target_size=(224, 224),\n",
    "                                             class_mode='categorical',\n",
    "                                             classes=class_subset,\n",
    "                                             batch_size=1,\n",
    "                                             shuffle=False,\n",
    "                                             seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c14ca3",
   "metadata": {},
   "source": [
    "# Using Pre-trained Layers for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25aa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    \"\"\"\n",
    "    Compiles a model integrated with VGG16 pretrained layers\n",
    "    \n",
    "    input_shape: tuple - the shape of input images (width, height, channels)\n",
    "    n_classes: int - number of classes for the output layer\n",
    "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
    "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
    "                If set to 0, all pretrained layers will freeze during training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
    "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                     weights='imagenet', \n",
    "                     input_shape=input_shape)\n",
    "    \n",
    "    # Defines how many layers to freeze during training.\n",
    "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
    "    # depending on the size of the fine-tuning parameter.\n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
    "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
    "    top_model = conv_base.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(4096, activation='relu')(top_model)\n",
    "    top_model = Dense(1072, activation='relu')(top_model)\n",
    "    top_model = Dense(1024 , activation='relu')(top_model)\n",
    "    top_model = Dense(1024 , activation='relu')(top_model)\n",
    "    top_model = Dense(512 , activation='relu')(top_model)\n",
    "    top_model = Dense(512 , activation='relu')(top_model)\n",
    "    top_model = Dense(256 , activation='relu')(top_model)\n",
    "    top_model = Dropout(0.2)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "    \n",
    "    \n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "    \n",
    "    # Confirms unfrozen layers \n",
    "    for layer in model.layers:\n",
    "        if layer.trainable==True:\n",
    "            print(layer)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fb11d",
   "metadata": {},
   "source": [
    "## Training with Fine-Tuning 4 layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baea8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 06:24:38.439682: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-23 06:24:38.440368: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x16e224a60>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x16e2017f0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x16e2173d0>\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x16e419940>\n",
      "<keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x16de6eaf0>\n",
      "<keras.layers.reshaping.flatten.Flatten object at 0x16e42c550>\n",
      "<keras.layers.core.dense.Dense object at 0x16b59e970>\n",
      "<keras.layers.core.dense.Dense object at 0x16b5a9790>\n",
      "<keras.layers.core.dense.Dense object at 0x16b5a9fa0>\n",
      "<keras.layers.core.dense.Dense object at 0x16b5b44c0>\n",
      "<keras.layers.core.dense.Dense object at 0x16bedd6a0>\n",
      "<keras.layers.core.dense.Dense object at 0x16bedd7c0>\n",
      "<keras.layers.core.dense.Dense object at 0x16bedda60>\n",
      "<keras.layers.regularization.dropout.Dropout object at 0x16b5a97f0>\n",
      "<keras.layers.core.dense.Dense object at 0x16e427400>\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1072)              4391984   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              1098752   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,276,084\n",
      "Trainable params: 16,640,820\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "optim_1 = Adam(learning_rate=0.00001)\n",
    "n_classes=4\n",
    "\n",
    "n_steps = traingen.samples // BATCH_SIZE\n",
    "\n",
    "n_val_steps = validgen.samples // BATCH_SIZE\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=4)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef4fb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot.inputs.keras import PlotLossesCallback\n",
    "\n",
    "plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "# ModelCheckpoint callback - save best weights\n",
    "tl_checkpoint_1 = ModelCheckpoint(filepath='vgg_16_finetune.weights.best.hdf5',\n",
    "                                  save_best_only=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "#EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bac03",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1a5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 06:24:39.674549: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-07-23 06:24:40.183474: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-23 06:24:40.738720: W tensorflow/core/framework/op_kernel.cc:1768] UNKNOWN: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1035, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 903, in wrapped_generator\n",
      "    for data in generator_fn():\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1050, in generator_fn\n",
      "    yield x[i]\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n",
      "    img = image_utils.load_img(\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/image_utils.py\", line 423, in load_img\n",
      "    img = pil_image.open(io.BytesIO(f.read()))\n",
      "\n",
      "  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/PIL/Image.py\", line 3186, in open\n",
      "    raise UnidentifiedImageError(\n",
      "\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\nTraceback (most recent call last):\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/PIL/Image.py\", line 3186, in open\n    raise UnidentifiedImageError(\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_6]]\n  (1) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\nTraceback (most recent call last):\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/PIL/Image.py\", line 3186, in open\n    raise UnidentifiedImageError(\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2151]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\nTraceback (most recent call last):\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/PIL/Image.py\", line 3186, in open\n    raise UnidentifiedImageError(\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_6]]\n  (1) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\nTraceback (most recent call last):\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/image_utils.py\", line 423, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n\n  File \"/Users/rhesaurus/miniforge3/envs/tensyflow/lib/python3.8/site-packages/PIL/Image.py\", line 3186, in open\n    raise UnidentifiedImageError(\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x2baba8900>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2151]"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "vgg_history = vgg_model.fit(traingen,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=validgen,\n",
    "                            steps_per_epoch=n_steps,\n",
    "                            validation_steps=n_val_steps,\n",
    "                            callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23964cdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'vgg_16_finetune.weights.best.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvgg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvgg_16_finetune.weights.best.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# initialize the best trained weights\u001b[39;00m\n\u001b[1;32m      3\u001b[0m true_classes \u001b[38;5;241m=\u001b[39m testgen\u001b[38;5;241m.\u001b[39mclasses\n\u001b[1;32m      4\u001b[0m class_indices \u001b[38;5;241m=\u001b[39m traingen\u001b[38;5;241m.\u001b[39mclass_indices\n",
      "File \u001b[0;32m~/miniforge3/envs/tensyflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tensyflow/lib/python3.8/site-packages/h5py/_hl/files.py:507\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds)\u001b[0m\n\u001b[1;32m    502\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    503\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    504\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    505\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    506\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 507\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/miniforge3/envs/tensyflow/lib/python3.8/site-packages/h5py/_hl/files.py:220\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    219\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 220\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    222\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'vgg_16_finetune.weights.best.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "vgg_model.load_weights('vgg_16_finetune.weights.best.hdf5') # initialize the best trained weights\n",
    "\n",
    "true_classes = testgen.classes\n",
    "class_indices = traingen.class_indices\n",
    "class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "\n",
    "vgg_preds = vgg_model.predict(testgen)\n",
    "vgg_pred_classes = np.argmax(vgg_preds, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad1a76",
   "metadata": {},
   "source": [
    "## Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbba261",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_acc = accuracy_score(true_classes, vgg_pred_classes)\n",
    "print(\"VGG16 Model Accuracy with Fine-Tuning: {:.2f}%\".format(vgg_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0294a2",
   "metadata": {},
   "source": [
    "## Calculating F-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  f1_score, classification_report, confusion_matrix\n",
    "f1 = f1_score(vgg_pred_classes, true_classes, average='weighted')\n",
    "\n",
    "print(classification_report(vgg_pred_classes, true_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ccce8",
   "metadata": {},
   "source": [
    "# Confusion Matrix Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "class_names = testgen.class_indices.keys()\n",
    "\n",
    "def plot_heatmap(y_true, y_pred, class_names, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        square=True, \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names,\n",
    "        fmt='d', \n",
    "        cmap=plt.cm.Blues,\n",
    "        cbar=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, figsize=(15, 7))\n",
    "  \n",
    "plot_heatmap(true_classes, vgg_pred_classes, class_names, ax1, title=\"Transfer Learning (VGG16) with Fine-Tuning\")    \n",
    "\n",
    "fig.suptitle(\"Confusion Matrix Model Comparison\", fontsize=24,y=0.99)\n",
    "fig.tight_layout(pad=8)\n",
    "fig.subplots_adjust(top=.9)\n",
    "plt.show()\n",
    "\n",
    "#epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import img_to_array, load_img\n",
    "\n",
    "\n",
    "model = Model(inputs=[vgg_model.input], outputs=vgg_model.layers[17].output)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "img_path = './mpox.png'\n",
    "\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ee3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from block1_conv1\n",
    "filters, biases = vgg_model.layers[13].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "n_filters, ix = 64, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(14, 14, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        if j == 0:\n",
    "            plt.imshow(f[:, :, j], cmap='Reds')\n",
    "        elif j == 1:\n",
    "            plt.imshow(f[:, :, j], cmap='Greens')\n",
    "        elif j == 2:\n",
    "            plt.imshow(f[:, :, j], cmap='Blues')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87690d4f",
   "metadata": {},
   "source": [
    "# Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[vgg_model.input], outputs=vgg_model.layers[1].output)\n",
    "feature_maps = model.predict(x)\n",
    "feature_maps.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39097fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# plot the 64 maps in an 8x8 squares\n",
    "square = 4\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='gnuplot2') # 'RdBu','PRGn' 'CMRmap' 'gnuplot2'\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# plot the 64 maps in an 8x8 squares\n",
    "square = 4\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='RdBu') # 'RdBu','PRGn' 'CMRmap' 'gnuplot2'\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb455c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[vgg_model.input], outputs=vgg_model.layers[13].output)\n",
    "feature_maps = model.predict(x)\n",
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# plot the 64 maps in an 8x8 squares\n",
    "square = 4\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='gnuplot2') # 'RdBu','PRGn' 'CMRmap' 'gnuplot2'\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# plot the 64 maps in an 8x8 squares\n",
    "square = 4\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='RdBu') # 'RdBu','PRGn' 'CMRmap' 'gnuplot2'\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477d692",
   "metadata": {},
   "source": [
    "# Prediction On An External Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import img_to_array, load_img\n",
    "from math import ceil\n",
    "\n",
    "img_path = 'mpox.png'\n",
    "\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "print('Input image shape:', x.shape)\n",
    "\n",
    "my_image = imread(img_path)\n",
    "imshow(my_image)\n",
    "classwise_result = vgg_model.predict(x)\n",
    "result = classwise_result[0][1]\n",
    "print(\"Prediction Score (In Percentage) That The Image Input Is Infected With Monkeypox Is:\", '%.2f' % (result*100), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fb844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4297a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
