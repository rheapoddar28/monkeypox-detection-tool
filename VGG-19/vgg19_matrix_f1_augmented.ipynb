{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, Input, Conv2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f844f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d3be3",
   "metadata": {},
   "source": [
    "# Preparing the training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec64707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "splitfolders.ratio(\"../Datasets/Original/\", # The location of dataset\n",
    "                   output=\"../Datasets/Datasets_Divided/\", # The output location\n",
    "                   seed=22, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of splited dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3887f9",
   "metadata": {},
   "source": [
    "### Real-time Data Augmentation through ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23622b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_generator = ImageDataGenerator(rotation_range=90, \n",
    "                                     brightness_range=[0.1, 0.7],\n",
    "                                     width_shift_range=0.5, \n",
    "                                     height_shift_range=0.5,\n",
    "                                     horizontal_flip=True, \n",
    "                                     vertical_flip=True,\n",
    "                                     preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa161d1",
   "metadata": {},
   "source": [
    "\"\"\" # Data Augmentation \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66370c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import *\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "datagen = ImageDataGenerator(        \n",
    "            rotation_range=45,\n",
    "            width_shift_range=0.2,  \n",
    "            height_shift_range=0.2,\n",
    "            brightness_range=[0.1, 0.7],\n",
    "            shear_range=0.2,        \n",
    "            zoom_range=0.3,        \n",
    "            horizontal_flip=True, \n",
    "            vertical_flip=True,\n",
    "            fill_mode='nearest', cval=125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091e50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_directory='../Datasets/Datasets_Divided/train/'\n",
    "base_directory_target='../Datasets/Augmented/train/'\n",
    "disease=os.listdir(base_directory)\n",
    "\n",
    "for y in range(4):\n",
    "    images=os.listdir(base_directory+str(disease[y]))\n",
    "    dataset=[]\n",
    "    for i ,image_name in enumerate(images):\n",
    "        if(image_name.split('.')[1]=='png'):\n",
    "            image=io.imread(base_directory+str(disease[y])+'/'+image_name)\n",
    "            image=Image.fromarray(image,'RGB')\n",
    "            dataset.append(np.array(image))\n",
    "\n",
    "    x=np.array(dataset)\n",
    "    i = 1\n",
    "    for batch in datagen.flow(x, batch_size=1,save_to_dir=base_directory_target+str(disease[y]),save_prefix='aug',save_format='png'):        \n",
    "        i += 1\n",
    "        if i > (4*(x.shape[0])):  \n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6a6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_directory='../Datasets/Datasets_Divided/val/'\n",
    "base_directory_target='../Datasets/Augmented/val/'\n",
    "disease=os.listdir(base_directory)\n",
    "\n",
    "for y in range(4):\n",
    "    images=os.listdir(base_directory+str(disease[y]))\n",
    "    dataset=[]\n",
    "    for i ,image_name in enumerate(images):\n",
    "        if(image_name.split('.')[1]=='png'):\n",
    "            image=io.imread(base_directory+str(disease[y])+'/'+image_name)\n",
    "            image=Image.fromarray(image,'RGB')\n",
    "            dataset.append(np.array(image))\n",
    "\n",
    "    x=np.array(dataset)\n",
    "    i = 1\n",
    "    for batch in datagen.flow(x, batch_size=1,save_to_dir=base_directory_target+str(disease[y]),save_prefix='aug',save_format='png'):        \n",
    "        i += 1\n",
    "        if i > (4*(x.shape[0])):  \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db65a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Normal', 'Monkeypox', 'Measles', 'Chickenpox']\n",
      "['Normal', 'Monkeypox', 'Measles', 'Chickenpox']\n",
      "['Normal', 'Monkeypox', 'Measles', 'Chickenpox']\n"
     ]
    }
   ],
   "source": [
    "train_data_dir=\"../Datasets/Augmented/train/\"\n",
    "test_data_dir=\"../Datasets/Datasets_Divided/test/\"\n",
    "val_data_dir=\"../Datasets/Augmented/val/\"\n",
    "\n",
    "class_names=sorted(os.listdir(\"../Datasets/Augmented/train/\"), reverse=True)[:4]\n",
    "class_names_test=sorted(os.listdir(\"../Datasets/Datasets_Divided/test/\"), reverse=True)[:4]\n",
    "class_names_val = sorted(os.listdir(\"../Datasets/Augmented/val/\"), reverse=True)[:4]\n",
    "print(class_names)\n",
    "print(class_names_test)\n",
    "print(class_names_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7435e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_subset = sorted(os.listdir(\"../Datasets/Augmented/train/\"), reverse=True)[:4] # Using only the first 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0494a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5115 images belonging to 4 classes.\n",
      "Found 600 images belonging to 4 classes.\n",
      "Found 81 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "traingen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               subset='training',\n",
    "                                               batch_size = BATCH_SIZE, \n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n",
    "\n",
    "validgen = train_generator.flow_from_directory(val_data_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n",
    "\n",
    "testgen = test_generator.flow_from_directory(test_data_dir,\n",
    "                                             target_size=(224, 224),\n",
    "                                             class_mode='categorical',\n",
    "                                             classes=class_subset,\n",
    "                                             batch_size=1,\n",
    "                                             shuffle=False,\n",
    "                                             seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c14ca3",
   "metadata": {},
   "source": [
    "# Using Pre-trained Layers for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25aa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    \"\"\"\n",
    "    Compiles a model integrated with VGG19 pretrained layers\n",
    "    \n",
    "    input_shape: tuple - the shape of input images (width, height, channels)\n",
    "    n_classes: int - number of classes for the output layer\n",
    "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
    "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
    "                If set to 0, all pretrained layers will freeze during training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
    "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
    "    conv_base = tf.keras.applications.vgg19.VGG19(include_top=False,\n",
    "                                                  weights='imagenet',\n",
    "                                                  input_tensor=None,\n",
    "                                                  input_shape=input_shape,\n",
    "                                                  pooling=None)\n",
    "    \n",
    "    top_model = conv_base.output\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(4096, activation='relu')(top_model)\n",
    "    top_model = Dense(1072, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.2)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "    \n",
    "    \n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fb11d",
   "metadata": {},
   "source": [
    "## Training VGG-19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baea8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 00:25:17.819318: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 00:25:17.820131: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1072)              4391984   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1072)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 4292      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 127,185,204\n",
      "Trainable params: 127,185,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "optim_1 = Adam(learning_rate=0.0001)\n",
    "n_classes=4\n",
    "\n",
    "n_steps = traingen.samples // BATCH_SIZE\n",
    "\n",
    "n_val_steps = validgen.samples // BATCH_SIZE\n",
    "\n",
    "n_epochs = 25\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "vgg_19_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n",
    "vgg_19_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef4fb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot.inputs.keras import PlotLossesCallback\n",
    "\n",
    "plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "# ModelCheckpoint callback - save best weights\n",
    "tl_checkpoint_1 = ModelCheckpoint(filepath='vgg_19.weights.best.hdf5',\n",
    "                                  save_best_only=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "#EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8af652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 00:25:19.561219: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-04-07 00:25:20.230275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/79 [>.............................] - ETA: 23:52 - loss: 3.9310 - accuracy: 0.3406"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "vgg_19_history = vgg_19_model.fit(traingen,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=validgen,\n",
    "                            steps_per_epoch=n_steps,\n",
    "                            validation_steps=n_val_steps,\n",
    "                            callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23964cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_19_model.load_weights('vgg_19.weights.best.hdf5') # initialize the best trained weights\n",
    "\n",
    "true_classes = testgen.classes\n",
    "class_indices = traingen.class_indices\n",
    "class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "\n",
    "vgg_19_preds = vgg_19_model.predict(testgen)\n",
    "vgg_19_pred_classes = np.argmax(vgg_19_preds, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbba261",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_19_acc = accuracy_score(true_classes, vgg_19_pred_classes)\n",
    "print(\"VGG19 Model Accuracy without Fine-Tuning: {:.2f}%\".format(vgg_19_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta_squared = 4\n",
    "\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    " \n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    " \n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    " \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    " \n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(vgg_19_pred_classes, true_classes, average='weighted')\n",
    "\n",
    "print(classification_report(vgg_19_pred_classes, true_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "class_names = testgen.class_indices.keys()\n",
    "\n",
    "def plot_heatmap(y_true, y_pred, class_names, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        square=True, \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names,\n",
    "        fmt='d', \n",
    "        cmap=plt.cm.Blues,\n",
    "        cbar=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, figsize=(15, 7))\n",
    "  \n",
    "plot_heatmap(true_classes, vgg_19_pred_classes, class_names, ax1, title=\"VGG-19 (Pre trained Model)\")    \n",
    "\n",
    "fig.suptitle(\"Confusion Matrix Model Comparison\", fontsize=24,y=0.99)\n",
    "fig.tight_layout(pad=8)\n",
    "fig.subplots_adjust(top=.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import img_to_array, load_img\n",
    "\n",
    "\n",
    "model = Model(inputs=[vgg_19_model.input], outputs=vgg_19_model.layers[17].output)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "img_path = '../mpox.png'\n",
    "\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ee3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from block1_conv1\n",
    "filters, biases = vgg_19_model.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "n_filters, ix = 64, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(14, 14, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        if j == 0:\n",
    "            plt.imshow(f[:, :, j], cmap='Reds')\n",
    "        elif j == 1:\n",
    "            plt.imshow(f[:, :, j], cmap='Greens')\n",
    "        elif j == 2:\n",
    "            plt.imshow(f[:, :, j], cmap='Blues')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[vgg_19_model.input], outputs=vgg_19_model.layers[17].output)\n",
    "model.summary()\n",
    "feature_maps = model.predict(x)\n",
    "feature_maps.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39097fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# plot the 64 maps in an 8x8 squares\n",
    "square = 4\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='gnuplot2') # 'RdBu','PRGn' 'CMRmap' 'gnuplot2'\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "# plot the 64 maps in an 8x8 squares\n",
    "square = 20\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='RdBu') # 'RdBu','PRGn' 'CMRmap' 'gnuplot2'\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb455c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d442e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d059d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d384c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
